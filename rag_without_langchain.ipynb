{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "380cbeab-262b-4df1-9600-7de81e56c2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "HUGGINGFACE_TOKEN = \"\" # ATTENTION: to get a token: https://huggingface.co/settings/tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbed5996-1bf5-4eaf-8bfc-c9dec7773ebc",
   "metadata": {},
   "source": [
    "To set up a Retrieval-Augmented Generation (RAG) system using your Mac with the Ollama platform, we‚Äôll need to handle a few main components:\n",
    "\n",
    "1.\tLLM Model (using Ollama): We‚Äôll call an open-source language model through Ollama for generation.\n",
    "2.\tData Storage and Retrieval: This component indexes and retrieves relevant documents in response to a query.\n",
    "3.\tApplication Logic: This ties together the retrieval and generation, handling inputs and outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16158b4e-656e-4212-b3cb-4e7b4b29ade0",
   "metadata": {},
   "source": [
    "# 1. LLM Models (using Ollama)\n",
    "- Download and install ollama from ollama.com\n",
    "- Install some LLM models in your machine:\n",
    "  ```ollama install <model_name>```\n",
    "- List the existing models\n",
    "- Consider a Model with Retrieval-Augmented Fine-Tuning (like LlamaIndex, GPT-Neo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8092257a-d867-44f3-8a1a-ac3f5e61bd73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                        ID              SIZE      MODIFIED     \n",
      "nomic-embed-text:latest     0a109f422b47    274 MB    7 days ago      \n",
      "llama2-uncensored:latest    44040b922233    3.8 GB    8 days ago      \n",
      "deepseek-r1:1.5b            a42b25d8c10a    1.1 GB    2 weeks ago     \n",
      "llama3.2:latest             a80c4f17acd5    2.0 GB    4 weeks ago     \n",
      "llava:latest                8dd30f6b0cb1    4.7 GB    7 months ago    \n",
      "llama3:latest               a6990ed6be41    4.7 GB    9 months ago    \n"
     ]
    }
   ],
   "source": [
    "!ollama list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "208597e3-a072-4702-97de-0d0a03c70956",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install ollama \n",
    "\n",
    "def ollama(model, system_prompt, user_prompt):\n",
    "    import ollama  # https://pypi.org/project/ollama/\n",
    "\n",
    "    try:\n",
    "        response = ollama.chat(model=model,\n",
    "                               messages=[{'role': 'system', 'content': system_prompt},\n",
    "                                         {'role': 'user', 'content': user_prompt}]\n",
    "        )\n",
    "        return response['message']['content']\n",
    "\n",
    "    except ollama.ResponseError as e:\n",
    "        print('Error:', e.error)\n",
    "        if e.status_code == 404:\n",
    "            ollama.pull(model)\n",
    "            print(\"Re-run this and it will work! We pulled the model for you!\") \n",
    "            return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3435d6f-2ad4-4a95-a68a-c182afa56fde",
   "metadata": {},
   "source": [
    "Testing call_ollama() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95e7ade2-7ec6-441c-9c4e-f290478ac613",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"llama3.2\"\n",
    "system_prompt = \"You are a helpful assistant.\"\n",
    "user_prompt = \"Say hello world as an emoji!\"\n",
    "\n",
    "response = ollama(model, system_prompt, user_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5e5e540-63a6-47a1-95a6-5d1bd1206c8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'üåé'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071fdb3a-a125-432e-8fb0-1a03aca9475b",
   "metadata": {},
   "source": [
    "## Web Scraper into markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1fd99493-535a-4cef-a283-d122b351f017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install crawl4ai\n",
    "# !crawl4ai-setup\n",
    "\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode\n",
    "\n",
    "async def url2md(url):\n",
    "    browser_conf = BrowserConfig(headless=True)  # Run in headless mode\n",
    "    run_conf = CrawlerRunConfig(cache_mode=CacheMode.BYPASS)\n",
    "\n",
    "    async with AsyncWebCrawler(config=browser_conf) as crawler:\n",
    "        result = await crawler.arun(url=url, config=run_conf)\n",
    "\n",
    "        if result.success:\n",
    "            return result.markdown_v2.raw_markdown  # Return extracted content\n",
    "        else:\n",
    "            return f\"Error: {result.error_message}\"  # Handle errors gracefully"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3bfc4a4-cb2a-4fdf-b618-4fb103b73a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_markdown(file_path, content):\n",
    "    try:\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(content)\n",
    "        return file_path\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving file: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080ee87e-9581-4262-925a-05b47dd3fc9e",
   "metadata": {},
   "source": [
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06de1d95-609e-4826-ae8e-59add6279528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INIT].... ‚Üí Crawl4AI 0.4.248\n",
      "[FETCH]... ‚Üì https://northwave-cybersecurity.com... | Status: True | Time: 0.99s\n",
      "[SCRAPE].. ‚óÜ Processed https://northwave-cybersecurity.com... | Time: 33ms\n",
      "[COMPLETE] ‚óè https://northwave-cybersecurity.com... | Status: True | Total: 1.03s\n"
     ]
    }
   ],
   "source": [
    "url = 'https://northwave-cybersecurity.com'\n",
    "markdown_website = asyncio.run(url2md(url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "317f1d06-2e2c-4dd9-94dd-01163e66cf9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17307"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(markdown_website)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "24ae7ace-f536-48b7-a142-fc6479033d77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'northwave_site.md'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = 'northwave_site.md'\n",
    "content = markdown_website\n",
    "save_markdown(file_path, content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed109f53-7268-40a0-947b-2c5ed595c4a9",
   "metadata": {},
   "source": [
    "# 2. Loading .pdf and .docx to a Data Storage (using FAISS IndexFlatL2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec57e78c-63cb-4b6c-bec5-be38ed6427b2",
   "metadata": {},
   "source": [
    "### Converting from .pdf to text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "34332506-79e4-4929-907e-ca03958b8dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pymupdf\n",
    "import fitz  # PyMuPDF for PDFs\n",
    "\n",
    "def pdf2txt(file_path):\n",
    "    text = \"\"\n",
    "    with fitz.open(file_path) as doc:\n",
    "        for page in doc:\n",
    "            text += page.get_text(\"text\")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73fe2470-8f00-46d5-9866-94bf980f050c",
   "metadata": {},
   "source": [
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a600e519-b83d-4a61-9cba-bb0d37e8f3d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22614"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = 'Masters_Graduation_Project_Plan___Tudor_Dragan.pdf'\n",
    "text_from_pdf = pdf2txt(file_path)\n",
    "len(text_from_pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f906327-c966-44a1-af14-a759efcf54a6",
   "metadata": {},
   "source": [
    "### Converting from .docx to text (using python-docx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4e059972-e18e-4ddd-8aaf-6482e3846c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install python-docx\n",
    "import docx\n",
    "\n",
    "def docx2txt(file_path):\n",
    "    doc = docx.Document(file_path)\n",
    "    text = \"\\n\".join([paragraph.text for paragraph in doc.paragraphs])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9bac0f-f59d-44e8-accb-a4da7449ad8b",
   "metadata": {},
   "source": [
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "64b8cac0-d02a-4ac0-baad-f01dbe83c0cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5477"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = 'ISACA Risk interview responses.docx'\n",
    "text_from_docx = docx2txt(file_path)\n",
    "len(text_from_docx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030e9c61-1747-4adb-82cc-3ef51ecd4508",
   "metadata": {},
   "source": [
    "### Reading a .md to text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a283e4cc-41c6-47d4-8413-bfbbf5cbfa65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def md2txt(file_path):\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            return file.read()\n",
    "    except FileNotFoundError:\n",
    "        return \"Error: File not found.\"\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50f1a95-7eba-421c-bf72-b4d4e7579f43",
   "metadata": {},
   "source": [
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "373d1880-109d-4783-acdd-d5ae38df33b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17307"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = 'northwave_site.md'\n",
    "text_from_md = md2txt(file_path)\n",
    "len(text_from_md)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d44277-8991-4145-908f-3dc8a7f5e464",
   "metadata": {},
   "source": [
    "### Converting .json into flatten list of strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "54886f6c-49ca-4246-896b-22b63f7c8038",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def json2txt(file_path):  \n",
    "    def _flatten_json(data, prefix=\"\"):\n",
    "        items = []\n",
    "        \n",
    "        if isinstance(data, dict):\n",
    "            for key, value in data.items():\n",
    "                new_prefix = f\"{prefix}.{key}\" if prefix else key\n",
    "                \n",
    "                if isinstance(value, (dict, list)):\n",
    "                    items.extend(_flatten_json(value, new_prefix))\n",
    "                else:\n",
    "                    items.append(f\"{new_prefix}: {str(value)}\")\n",
    "                    \n",
    "        elif isinstance(data, list):\n",
    "            for i, item in enumerate(data):\n",
    "                new_prefix = f\"{prefix}[{i}]\"\n",
    "                \n",
    "                if isinstance(item, (dict, list)):\n",
    "                    items.extend(_flatten_json(item, new_prefix))\n",
    "                else:\n",
    "                    items.append(f\"{new_prefix}: {str(item)}\")\n",
    "                    \n",
    "        return items\n",
    "    \n",
    "    # Read the JSON file\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            json_data = json.load(file)\n",
    "        return \"\\n\".join(_flatten_json(json_data))\n",
    "        \n",
    "    except Exception as e:\n",
    "        return [f\"Error: {e}\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113c5cbe-e13f-4d04-9c6b-0d35f8b426fc",
   "metadata": {},
   "source": [
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "92c2fc1b-789a-4b96-b6f3-a29ed2b012b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "245"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = 'text.json'\n",
    "text_from_json = json2txt(file_path)\n",
    "len(text_from_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ea3654b1-e2f3-4bef-abd5-1610658c201d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'user.name: John Doe\\nuser.age: 30\\nuser.interests[0]: programming\\nuser.interests[1]: music\\nuser.interests[2]: sports\\norders[0].id: 1\\norders[0].product: laptop\\norders[0].price: 999.99\\norders[1].id: 2\\norders[1].product: phone\\norders[1].price: 599.99'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_from_json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ec8003-148c-4cd9-b497-68d47d2648d7",
   "metadata": {},
   "source": [
    "### Fetching ETDA data (https://apt.etda.or.th/cgi-bin/aptgroups.cgi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f6e5551e-e289-4daf-8399-e0da40cc65c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "etda_actors = requests.get('https://apt.etda.or.th/cgi-bin/getmisp.cgi?o=g')\n",
    "etda_actors = etda_actors.json()\n",
    "\n",
    "with open(\"etda_actors.json\", \"w\", encoding=\"utf-8\") as file:\n",
    "    json.dump(etda_actors, file, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b0254313-4026-4f1a-8b58-40f5669c898c",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'etda_actors.json'\n",
    "text_from_etda_actors = json2txt(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "33007879-4c2a-4d65-b216-8d87086a99ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "etda_actor_cards = requests.get('https://apt.etda.or.th/cgi-bin/getcard.cgi?g=all&o=j')\n",
    "etda_actor_cards = etda_actor_cards.json()\n",
    "\n",
    "with open(\"etda_actor_cards.json\", \"w\", encoding=\"utf-8\") as file:\n",
    "    json.dump(etda_actor_cards, file, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e92793af-9365-4899-b684-f3b44185f21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'etda_actor_cards.json'\n",
    "text_from_etda_actors_cards = json2txt(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb12898-9462-44b2-8ece-41412ef01d50",
   "metadata": {},
   "source": [
    "### Fetching MITRE Groups (https://attack.mitre.org/groups/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "44a546bf-5b42-4f91-ae26-db2a85c059cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "mitre_actors = requests.get('https://raw.githubusercontent.com/mitre/cti/master/enterprise-attack/enterprise-attack.json')\n",
    "mitre_actors = mitre_actors.json()\n",
    "\n",
    "with open(\"mitre_actors.json\", \"w\", encoding=\"utf-8\") as file:\n",
    "    json.dump(mitre_actors, file, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "26d2e5b7-c8c9-4df2-9539-7f3728006041",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'mitre_actors.json'\n",
    "text_from_mitre_actors = json2txt(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed4450b-3054-48c2-a969-00ea251814f6",
   "metadata": {},
   "source": [
    "## Creating the text embeddings using a pre-trained transformer model. I.e. text to token to vector!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85823ba9-ea03-453b-8aeb-3d01fbc33464",
   "metadata": {},
   "source": [
    "Check: https://huggingface.co/sentence-transformers\n",
    "\n",
    "- sentence-transformers/all-MiniLM-L6-v2\n",
    "- sentence-transformers/all-mpnet-base-v2 : MPNet, which is a transformer model similar to BERT. Hidden size of 768, meaning that each token in the input gets mapped to a 768-dimensional vector. No matter how many words you feed, the final representation is always 768-dimensional because it represents the compressed meaning of the sentence in a fixed-size vector.\n",
    "- distilbert-base-uncased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3ebe48a6-667e-4d6f-b688-7f811997734c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(HUGGINGFACE_TOKEN) # ATTENTION: to get a token: https://huggingface.co/settings/tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d9c37ae1-064e-47d8-b335-3e360e36aca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch ipywidgets\n",
    "# !pip install -U transformers sentence-transformers\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "model_name = \"sentence-transformers/all-mpnet-base-v2\"  \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "def text2embedding(text):\n",
    "    # Tokenize input text and convert to tensor\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512) #returns PyTorch tensors\n",
    "    \n",
    "    # Forward pass through the model to get hidden states\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Get the embeddings from the last hidden layer\n",
    "    hidden_states = outputs.last_hidden_state  # Shape: (1, seq_len, hidden_dim)\n",
    "    \n",
    "    # Average pool over the token embeddings to get a single vector\n",
    "    embedding = hidden_states.mean(dim=1).squeeze().numpy()  # Shape: (hidden_dim,)\n",
    "    \n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1415556-7a67-43c5-af7c-bbee22ed6c75",
   "metadata": {},
   "source": [
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5030716f-b912-4818-ab7f-069dfd51d7fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding1 = text2embedding(text_from_docx)\n",
    "len(embedding1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3385fe30-4887-4f03-94fa-4fca24e9e3f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding2 = text2embedding(text_from_md)\n",
    "len(embedding2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "12b64e00-0f39-467e-8df5-aabc97ef8386",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding3 = text2embedding(text_from_pdf)\n",
    "len(embedding3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "72f480f3-b1f0-4a3b-84a5-6cf53a488bb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding4 = text2embedding(text_from_json)\n",
    "len(embedding4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bdb585f1-c54e-4c0e-b99e-805eb7fb2488",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding5 = text2embedding(text_from_etda_actors)\n",
    "len(embedding5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3c68fc54-4ae1-47ef-8116-5ba45735cee5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding6 = text2embedding(text_from_etda_actors_cards)\n",
    "len(embedding6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fcbc2368-4383-457f-b80f-a490fed2cbb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding7 = text2embedding(text_from_mitre_actors)\n",
    "len(embedding7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5add5f-ef18-45b6-89e7-e646d8268862",
   "metadata": {},
   "source": [
    "## The vector database (for similarity search)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c962c7b-2835-4b0b-a7cc-846a2fb8b574",
   "metadata": {},
   "source": [
    "1. FAISS (Facebook AI Similarity Search)\n",
    "2. Weaviate\n",
    "3. Pinecone\n",
    "4. ChromaDB\n",
    "5. Milvus\n",
    "6. Qdrant\n",
    "7. LanceDB (anythingLLM)\n",
    "8. Zilliz Cloud\n",
    "9. Arrant \n",
    "10. AstraDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1acda0c2-dc7e-4916-ae55-2aa0e8305841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install faiss-cpu\n",
    "import faiss\n",
    "dimension = 768 #check the dimension of your embeddings\n",
    "index = faiss.IndexFlatL2(dimension) #Types of index: IndexFlatL2, IndexIVFFlat, IndexHNSWFlat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ce9681-06ec-494c-bce6-f4061eba74ff",
   "metadata": {},
   "source": [
    "### Loading files from a folder into the database (FAISS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7860c32d-72bc-4092-97f6-5fdef12f3495",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def load_and_index_files(folder_path, index):\n",
    "    documents = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        \n",
    "        # Check file extension and load text\n",
    "        if filename.endswith(\".pdf\"):\n",
    "            text = pdf2txt(file_path)\n",
    "        elif filename.endswith(\".docx\"):\n",
    "            text = docx2txt(file_path)\n",
    "        elif filename.endswith(\".json\"):\n",
    "            text = json2txt(file_path)\n",
    "        elif filename.endswith(\".md\"):\n",
    "            text = md2txt(file_path)\n",
    "        else:\n",
    "            continue  # Skip non-supported files\n",
    "        \n",
    "        # Embed and add to index\n",
    "        embedding = text2embedding(text)\n",
    "        index.add(np.array([embedding]))\n",
    "        \n",
    "        # Track document content and metadata (optional)\n",
    "        documents.append({\"filename\": filename, \"content\": text, \"embedding\": embedding})\n",
    "    \n",
    "    return documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74fd4e60-d86a-4fc0-a0e1-9c07bb57560f",
   "metadata": {},
   "source": [
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ccafa14b-5a49-437b-8723-1ce4ec541347",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total documents indexed: 7\n"
     ]
    }
   ],
   "source": [
    "folder_path = '.'\n",
    "documents = load_and_index_files(folder_path, index)\n",
    "print(f\"Total documents indexed: {len(documents)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762c6e1a-5fb5-48ef-aef2-fbe5db18dbd3",
   "metadata": {},
   "source": [
    "### Function to search for documents similar (top_k elements, KNN) to the query/user_prompt converted to embedding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ce17eb80-065b-47c2-bf98-836997db7191",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_documents(query_embedding, index, documents, top_k=3):\n",
    "    # Search for the nearest neighbors\n",
    "    distances, indices = index.search(np.array([query_embedding], dtype='float32'), top_k)\n",
    "    \n",
    "    # Print indices for debugging\n",
    "    # print(f\"indices: {indices}, distances: {distances}\")\n",
    "    \n",
    "    # Ensure indices is structured correctly and has expected dimensions\n",
    "    if indices.size == 0 or indices[0][0] == -1:\n",
    "        print(\"No matching documents found.\")\n",
    "        return []  # Return empty list if no results are found\n",
    "\n",
    "    print(\"Match:\",[documents[i]['filename'] for i in indices[0] if i < len(documents)])\n",
    "    # Fetch top_k documents using indices\n",
    "    return [documents[i] for i in indices[0] if i < len(documents)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6cd3c8-3f0c-4497-b80c-928fdd27126e",
   "metadata": {},
   "source": [
    "# Create the RAG Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0559ffa5-d2a8-4628-82a6-a30ceb3e8b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_pipeline(query, model=\"llama3.2\", top_k=1):\n",
    "    query_embedding = text2embedding(query) #Convert the query/prompt into an embedding (vector)\n",
    "\n",
    "    relevant_docs = retrieve_documents(query_embedding, index, documents, top_k)\n",
    "    # print(\"Relevant docs:\",relevant_docs)\n",
    "    \n",
    "    # 3. Construct the prompt with file information\n",
    "    context = \"\\n\".join([f\"File: {doc['filename']}\\nContent:\\n{doc['content']}\" for doc in relevant_docs])\n",
    "    # print(\"Context:\",context)\n",
    "    \n",
    "    system_prompt = \"\"\"Given the following context from relevant documents, and a follow up question, reply with an answer to the current question the user is asking. \n",
    "    Return only your response to the question given the above information following the users instructions as needed.\n",
    "    Answer strictly based on the provided context. \n",
    "    If the context does not include any information relevant to the question, respond exactly with \"The context does not provide information on this topic.\"\"\"\n",
    "    \n",
    "    user_prompt = f\"\"\"Context (from relevant documents):{context}   \n",
    "    \n",
    "    Question: {query} \n",
    "    \"\"\"\n",
    "    # print(\"User prompt:\",user_prompt)\n",
    "\n",
    "    # 4. Call the language model\n",
    "    response = ollama(model,system_prompt, user_prompt)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870a99a9-4026-4c98-9ec9-54278cc4eb6c",
   "metadata": {},
   "source": [
    "# ==============================================================================\n",
    "# ==============================================================================\n",
    "# Testing everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "14eca494-ddcc-4fb7-b1fa-f6fe2ca6becb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Match: ['etda_actors.json']\n",
      "From the provided text, I can gather the following information about \"Zombie Spider\":\n",
      "\n",
      "1. **Identity**: Zombie Spider is believed to be a pseudonym for Peter Yuryevich LEVASHOV, a Russian national who was involved in operating malware and botnets.\n",
      "2. **Background**: LEVASHOV was arrested in Spain in April 2017 when the final version of Kelihos was taken over. He recently pleaded guilty to operating the botnet for criminal purposes.\n",
      "3. **Malware distribution**: Zombie Spider (LEVASHOV) has been linked to distributing various malware, including:\n",
      "\t* TrickBot\n",
      "\t* Zeus Panda ('Bamboo Spider')\n",
      "4. **Criminal activities**: In the past, LEVASHOV was involved in other malicious activities such as:\n",
      "\t* Pump-and-dump stock scams\n",
      "\t* Date ruses\n",
      "\t* Credential phishing\n",
      "\t* Money mule recruitment\n",
      "\t* Rogue online pharmacy advertisements\n",
      "\n",
      "5. **Botnet takeovers**: Zombie Spider has been associated with taking over various botnets, including the Kelihos botnet.\n",
      "\n",
      "6. **Takedown**: In 2017, CrowdStrike reported on the takedown of Zombie Spider and the Kelihos botnet.\n",
      "\n",
      "These details provide insight into the malicious activities associated with Zombie Spider (LEVASHOV) and his involvement in operating malware and botnets for criminal purposes.\n"
     ]
    }
   ],
   "source": [
    "query = \"What do you know about 'Zombie Spider' from the ETDA document?\"\n",
    "response = rag_pipeline(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f5df63be-c271-47db-8b69-9c972b8bd111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Match: ['northwave_site.md']\n",
      "Based on the provided content, here is a list of URL references related to \"Zombie Spider\" from the ETDA (European Union's Network and Information Security Directive) document:\n",
      "\n",
      "1. https://northwave-cybersecurity.com/ (General website URL)\n",
      "2. https://northwave-cybersecurity.com/responsible-disclosure (Responsible Disclosure page)\n",
      "3. https://northwave-cybersecurity.com/privacy-cookie-statement (Privacy Statement)\n",
      "4. https://northwave-cybersecurity.com/cookie-statement (Cookie Statement)\n",
      "\n",
      "Note that the \"Zombie Spider\" is likely a reference to the \"Zombie Spider Attack\" mentioned in the ETDA document, which is a type of cyber attack. However, without more specific information about the content of the ETDA document, it's difficult to provide a more detailed list of URL references related to this topic.\n",
      "\n",
      "Please let me know if you'd like me to help with anything else!\n"
     ]
    }
   ],
   "source": [
    "query = \"What are the list of url references on 'Zombie Spider' from the ETDA document?\"\n",
    "response = rag_pipeline(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39501740-39b0-4045-9870-6ce9243d59ad",
   "metadata": {},
   "source": [
    "# ‚Üëwrong!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "44e7281e-7430-4c19-81c5-aeef234fb414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Match: ['etda_actors.json']\n",
      "According to the etda actors document, the reference related to 'Zombie Spider' is:\n",
      "\n",
      "https://apt.etda.or.th/cgi-bin/showcard.cgi?u=2c1d1677-f2d9-44e1-ac9a-4f7f4047e2d5\n"
     ]
    }
   ],
   "source": [
    "query = \"From the etda actors document, list the references related to 'Zombie Spider'\"\n",
    "response = rag_pipeline(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a2278884-89d0-4df4-9171-4e47941eb408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Match: ['etda_actors.json']\n",
      "Based on the provided document from ETDa Actors, the references (refs) related to the \"Zombie Spider\" actor are:\n",
      "\n",
      "* https://apt.etda.or.th/cgi-bin/showcard.cgi?u=2c1d1677-f2d9-44e1-ac9a-4f7f4047e2d5\n",
      "* https://www.crowdstrike.com/blog/inside-the-takedown-of-zombie-spider-and-the-kelihos-botnet/\n",
      "* https://www.crowdstrike.com/blog/farewell-to-kelihos-and-zombie-spider/\n",
      "\n",
      "These references are directly related to the \"Zombie Spider\" actor, specifically mentioning Kelihos and the takedown of the botnet.\n"
     ]
    }
   ],
   "source": [
    "query = \"From the etda actors document, list the refs related to 'Zombie Spider' actor\"\n",
    "response = rag_pipeline(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687861f4-688c-436a-a1d8-4fc81cc6a871",
   "metadata": {},
   "source": [
    "# ‚Üënot complete!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9e12afed-ded4-4ce0-aaa7-2200ba82c630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Match: ['northwave_site.md']\n",
      "According to the provided website content, Northwave appears to be a cybersecurity company or consulting firm. The text does not explicitly state that it is a specific product or service, but rather a company that offers various cybersecurity-related services.\n",
      "\n",
      "As for its location, according to the contact information provided on the website, Northwave is located in:\n",
      "\n",
      "* Netherlands: Van Deventerlaan 31-51, 3528 AG Utrecht\n",
      "* Postal address: PO 1305, 3430 BH, Nieuwegein\n",
      "\n",
      "Note that there is no explicit mention of Northwave being a specific product or service, but rather an organization that offers various cybersecurity-related services.\n"
     ]
    }
   ],
   "source": [
    "query = \"From the Northwave site document, tell me what is northwave? and where Northwave is located?\"\n",
    "response = rag_pipeline(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ace13d79-beb9-4895-9226-39269c1340bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Match: ['northwave_site.md']\n",
      "Based on the provided website content, I found that:\n",
      "\n",
      "* The physical address is:\n",
      "Van Deventerlaan 31-51, 3528 AG Utrecht\n",
      "* The postal address is:\n",
      "PO 1305, 3430 BH, Nieuwegein\n",
      "\n",
      "Unfortunately, I could not find the contact phone number on the website.\n"
     ]
    }
   ],
   "source": [
    "query = \"From the Northwave site document, could you get the address and the contact phone of Northwave? \"\n",
    "response = rag_pipeline(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f91442f-ca23-460b-b865-5fe6bbee7a1d",
   "metadata": {},
   "source": [
    "# ‚ÜëWrong!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "79d83a72-b87a-468e-bbf1-2af63ee22955",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Match: ['northwave_site.md']\n",
      "Based on the provided Northwave website content, here are their listed services:\n",
      "\n",
      "**Business**\n",
      "\n",
      "1. Managed Security & Privacy Office\n",
      "2. State of Security Assessment\n",
      "3. Data Protection Impact Assessment\n",
      "4. Security Roadmap\n",
      "5. Assess & Control\n",
      "6. ISO 27001 FastTrack\n",
      "\n",
      "**Bytes**\n",
      "\n",
      "1. Managed Detection & Response\n",
      "2. Rapid Response\n",
      "3. Red Teaming\n",
      "4. Advanced Red Teaming (ART)\n",
      "5. Penetration Testing (Pentest)\n",
      "6. Vulnerability Management\n",
      "\n",
      "**Behaviour**\n",
      "\n",
      "1. Managed Cyber Behaviour\n",
      "2. Cyber Resilience Service\n"
     ]
    }
   ],
   "source": [
    "query = \"From the Northwave site document, could you list northwave's services?\"\n",
    "response = rag_pipeline(query)\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
