{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60bc4fa3-a719-4980-a06a-afb3bdc59c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain ollama langchain-ollama faiss-cpu pypdf python-docx markdown beautifulsoup4 pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a6500fa-cc29-494b-9b79-8b3fb4258859",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import faiss\n",
    "import numpy as np\n",
    "# from langchain.embeddings import OllamaEmbeddings\n",
    "from langchain_ollama import OllamaEmbeddings \n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from langchain.document_loaders import PyPDFLoader, UnstructuredWordDocumentLoader\n",
    "from bs4 import BeautifulSoup\n",
    "from docx import Document as DocxDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b513184-fda9-4827-8317-bc7b4644cee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = OllamaEmbeddings(model=\"nomic-embed-text\")  # Change to preferred model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b4a3b857-b5d8-405f-b36e-3999c3ad051e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_text = \"This is a test embedding\"\n",
    "len(embedding_model.embed_query(sample_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d1efefa-1d44-47df-a3b2-b504fb85c945",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory to store FAISS index\n",
    "DB_PATH = \"vector_db\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96bd322f-33fd-45d9-a237-bc66f244d016",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(text, source):\n",
    "    \"\"\"Splits text into chunks and converts them into LangChain Documents.\"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "    chunks = text_splitter.split_text(text)\n",
    "    \n",
    "    return [Document(page_content=chunk, metadata={\"source\": source}) for chunk in chunks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96f0dcb7-916b-4ab2-b066-4624c3aea6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pdf(file_path):\n",
    "    \"\"\"Extracts text from a PDF and converts it into embeddings.\"\"\"\n",
    "    loader = PyPDFLoader(file_path)\n",
    "    pages = loader.load()\n",
    "    text = \"\\n\".join([page.page_content for page in pages])\n",
    "    return process_text(text, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "713e6215-5fcc-41f7-96c6-cb5381eca174",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_docx(file_path):\n",
    "    \"\"\"Extracts text (including tables) from a DOCX file and converts it into embeddings.\"\"\"\n",
    "    doc = DocxDocument(file_path)\n",
    "    \n",
    "    text = []\n",
    "    \n",
    "    # Extract paragraphs\n",
    "    for para in doc.paragraphs:\n",
    "        if para.text.strip():  # Ignore empty lines\n",
    "            text.append(para.text.strip())\n",
    "    \n",
    "    # Extract tables\n",
    "    for table in doc.tables:\n",
    "        for row in table.rows:\n",
    "            row_text = [cell.text.strip() for cell in row.cells if cell.text.strip()]\n",
    "            if row_text:\n",
    "                text.append(\" | \".join(row_text))  # Format as a readable string\n",
    "    \n",
    "    full_text = \"\\n\".join(text)\n",
    "    \n",
    "    if not full_text.strip():\n",
    "        print(\"‚ö†Ô∏è No text extracted from the DOCX file. It might be empty or contain only images.\")\n",
    "        return []\n",
    "    \n",
    "    return process_text(full_text, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40add2d7-c35e-4098-a831-0ad740d138e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json(file_path):\n",
    "    \"\"\"Extracts text from a JSON file and converts it into embeddings.\"\"\"\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    text = json.dumps(data, indent=2)  # Convert JSON to string\n",
    "    return process_text(text, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2870745a-5155-4813-94ff-6a0c19f33668",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_md(file_path):\n",
    "    \"\"\"Extracts text from a Markdown file and converts it into embeddings.\"\"\"\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "    \n",
    "    return process_text(text, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e263b812-8e90-49fb-a607-a5df70ee7146",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_html(file_path):\n",
    "    \"\"\"Extracts text from an HTML file and converts it into embeddings.\"\"\"\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        soup = BeautifulSoup(f, \"html.parser\")\n",
    "    \n",
    "    text = soup.get_text()\n",
    "    return process_text(text, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "890ef3a2-50c8-42d0-aaa9-4d32c8cf4411",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_or_load_db():\n",
    "    \"\"\"Creates or loads an existing FAISS vector database with safe deserialization.\"\"\"\n",
    "    if os.path.exists(DB_PATH):\n",
    "        return FAISS.load_local(DB_PATH, embedding_model, allow_dangerous_deserialization=True)  # ‚úÖ Fix applied\n",
    "\n",
    "    return None  # Return None if DB doesn't exist yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "acedd380-deff-4745-aa73-0191d2028c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "\n",
    "def compute_document_hash(text):\n",
    "    \"\"\"Computes a unique hash for a document based on its content.\"\"\"\n",
    "    return hashlib.md5(text.encode()).hexdigest()  # MD5 hash of the document text\n",
    "\n",
    "def add_to_db(docs):\n",
    "    \"\"\"Adds documents to the FAISS vector store only if they are not already stored.\"\"\"\n",
    "    if not docs:\n",
    "        print(\"‚ö†Ô∏è No documents to add to the database.\")\n",
    "        return\n",
    "\n",
    "    db = create_or_load_db()\n",
    "    \n",
    "    if db is None:\n",
    "        print(\"‚ÑπÔ∏è No existing database found. Creating a new one.\")\n",
    "        db = FAISS.from_documents(docs, embedding_model)\n",
    "        db.save_local(DB_PATH)\n",
    "        print(f\"‚úÖ Indexed {len(docs)} new chunks.\")\n",
    "        return\n",
    "\n",
    "    # Retrieve existing document hashes\n",
    "    existing_hashes = set(\n",
    "        doc.metadata.get(\"hash\") for doc in db.docstore._dict.values() if \"hash\" in doc.metadata\n",
    "    )\n",
    "\n",
    "    # Filter out duplicate documents\n",
    "    new_docs = []\n",
    "    for doc in docs:\n",
    "        doc_hash = compute_document_hash(doc.page_content)\n",
    "        if doc_hash not in existing_hashes:\n",
    "            doc.metadata[\"hash\"] = doc_hash  # Store the hash in metadata\n",
    "            new_docs.append(doc)\n",
    "        # else:\n",
    "        #     print(f\"‚ö†Ô∏è Skipping duplicate document: {doc.metadata.get('source', 'Unknown')}\")\n",
    "\n",
    "    if new_docs:\n",
    "        db.add_documents(new_docs)\n",
    "        db.save_local(DB_PATH)\n",
    "        print(f\"‚úÖ Indexed {len(new_docs)} new chunks.\")\n",
    "    else:\n",
    "        print(\"‚ÑπÔ∏è No new documents to add.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5523fc49-bc52-4ffe-9206-bc55407b9bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_db(query, k=3):\n",
    "    \"\"\"Retrieves the most relevant paragraphs from the database.\"\"\"\n",
    "    db = create_or_load_db()\n",
    "    results = db.similarity_search(query, k=k)\n",
    "    \n",
    "    for i, doc in enumerate(results):\n",
    "        print(f\"\\nüîπ Result {i+1} (Source: {doc.metadata['source']}):\\n{doc.page_content}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f2bb416f-ae1f-4eb3-ad31-b14a299348f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_store(file_path):\n",
    "    \"\"\"Determines the file type and processes it accordingly.\"\"\"\n",
    "    ext = file_path.split(\".\")[-1].lower()\n",
    "    \n",
    "    if ext == \"pdf\":\n",
    "        docs = load_pdf(file_path)\n",
    "    elif ext == \"docx\":\n",
    "        docs = load_docx(file_path)\n",
    "    elif ext == \"json\":\n",
    "        docs = load_json(file_path)\n",
    "    elif ext == \"md\":\n",
    "        docs = load_md(file_path)\n",
    "    elif ext == \"html\":\n",
    "        docs = load_html(file_path)\n",
    "    else:\n",
    "        print(\"‚ùå Unsupported file format!\")\n",
    "        return\n",
    "    \n",
    "    add_to_db(docs)\n",
    "    print(f\"‚úÖ Successfully indexed {len(docs)} chunks from {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "90a505b6-e6ba-4d41-826e-ff04e575b316",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_relevant_paragraphs(query, k=3):\n",
    "    \"\"\"Retrieves the most relevant paragraphs from the database.\"\"\"\n",
    "    db = create_or_load_db()\n",
    "    \n",
    "    if db is None:\n",
    "        return \"‚ö†Ô∏è No database found. Please add documents first.\"\n",
    "    \n",
    "    results = db.similarity_search(query, k=k)\n",
    "    \n",
    "    if not results:\n",
    "        return \"‚ö†Ô∏è No relevant results found in the database.\"\n",
    "    \n",
    "    context = \"\\n\\n\".join([f\"Source: {doc.metadata['source']}\\n{doc.page_content}\" for doc in results])\n",
    "    \n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2b00f947-3fe7-464a-9c27-a843168d8379",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_with_rag(query):\n",
    "    \"\"\"Retrieves relevant paragraphs and generates an answer using Llama 3.2.\"\"\"\n",
    "    context = retrieve_relevant_paragraphs(query)\n",
    "    \n",
    "    if \"‚ö†Ô∏è\" in context:\n",
    "        return context  # Return warning message if no context is found\n",
    "    \n",
    "    prompt = f\"\"\"You are a helpful assistant using retrieval-augmented generation (RAG). Answer the following question based on the retrieved documents.\n",
    "\n",
    "    Context:\n",
    "    {context}\n",
    "\n",
    "    Question: {query}\n",
    "\n",
    "    Answer:\"\"\"\n",
    "    \n",
    "    response = llm.invoke(prompt)\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6190f4d2-0beb-47ac-b091-cd54e2f79081",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "\n",
    "# Initialize Llama 3.2 via Ollama\n",
    "llm = Ollama(model=\"llama3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495c6782-7199-4071-ad01-1177bf93a773",
   "metadata": {},
   "source": [
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "01b3cd7e-9d1c-44fb-89ae-21366d55c430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Successfully indexed 17 chunks from ISACA Risk interview responses.docx\n"
     ]
    }
   ],
   "source": [
    "process_and_store(\"ISACA Risk interview responses.docx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ac2bac11-dc9a-46d1-af7c-cd8810a67f2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'According to the provided document, \"ISACA Risk interview responses. docx\", the main topic of the document is the upcoming presentation at the ISACA Risk Event titled \"The Double-Edged Sword: Risks of AI Language Models in Cybersecurity\".'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_with_rag(\"What is the main topic of the document?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f3dffb6d-256d-44c1-85e0-55577fb8f823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the retrieved documents (ISACA Risk interview responses), I found that the answer to your question is:\n",
      "\n",
      "The presentation will be given by ISACA.\n",
      "\n",
      "Additionally, according to the retrieved documents, some practical steps businesses can take to prevent becoming too dependent on AI and complement AI solutions with human expertise are:\n",
      "\n",
      "1. Establish clear governance and oversight: Define roles and responsibilities for AI deployment and ensure accountability.\n",
      "2. Develop diverse AI training data: Incorporate a wide range of scenarios, cases, and perspectives to reduce biases and improve AI decision-making.\n",
      "3. Implement transparent AI decision-making processes: Ensure AI-driven decisions are explainable, auditable, and subject to human review.\n",
      "4. Foster collaboration between humans and AI: Encourage joint decision-making and continuous learning from each other's strengths and limitations.\n",
      "5. Continuously monitor and evaluate AI performance: Regularly assess AI performance, identify biases or errors, and make adjustments as needed.\n",
      "\n",
      "Please note that these are general suggestions based on the retrieved documents and may not be a comprehensive answer to your specific question.\n"
     ]
    }
   ],
   "source": [
    "query = \"Who will give the presentation?\"\n",
    "print(chat_with_rag(query))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683c7e6d-466d-43ac-a535-ec1cc3a147d3",
   "metadata": {},
   "source": [
    "# ‚ÜëWrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6814b7e8-5ced-4957-a562-5b318dd8eaee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ÑπÔ∏è No new documents to add.\n",
      "‚úÖ Successfully indexed 48 chunks from northwave_site.md\n"
     ]
    }
   ],
   "source": [
    "process_and_store(\"northwave_site.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f31ebe51-81fb-4ba3-bcc7-7956375ace97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the retrieved documents, I found that the Northwave site offers Cyber Security Services. The specific services are not explicitly listed in the provided context, but you can visit their website at https://northwave-cybersecurity.com/cyber-security-services to learn more about what they have to offer.\n"
     ]
    }
   ],
   "source": [
    "query = \"From the Northwave site, what are the services provided by northwave?\"\n",
    "print(chat_with_rag(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "385cb6bf-1f89-42c4-9df2-d78ac77346db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the retrieved documents, I found that the address of Northwave can be found at the following link:\n",
      "\n",
      "https://northwave-cybersecurity.com/cyber-security-services?hsLang=en\n",
      "\n",
      "This is where you can find their Cyber Security Services page.\n"
     ]
    }
   ],
   "source": [
    "query = \"What is the address of Northwave?\"\n",
    "print(chat_with_rag(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "556f86c6-b53d-4244-87b1-b41974d93de7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the retrieved documents, I found that there is no explicit information about the location of Northwave in the provided sources (northwave_ site.md). However, it's possible to infer from the links provided that Northwave may be a cyber security company with an online presence. If you're looking for more specific information about their location, I recommend checking out other credible sources or reaching out to them directly for clarification.\n"
     ]
    }
   ],
   "source": [
    "query = \"What is Northwave is located?\"\n",
    "print(chat_with_rag(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7e88b0b7-62cd-4d25-82c6-ca69508c1acd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the retrieved documents, Northwave's phone number is +31 (0)30 303 1240.\n"
     ]
    }
   ],
   "source": [
    "query = \"What is Northwave's phone number?\"\n",
    "print(chat_with_rag(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "49d8b350-a889-4a2e-85b8-02e80aee9908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the retrieved documents, I found that Northwave Cybersecurity has multiple language options for their website, including English, Deutsch (German), Nederlands (Dutch), and Svenska (Swedish). However, there is no explicit mention of the country where Northwave is located.\n",
      "\n",
      "However, considering the language options provided, it's likely that Northwave is a company based in Europe or has strong ties to European countries.\n"
     ]
    }
   ],
   "source": [
    "query = \"In what country Northwave is located?\"\n",
    "print(chat_with_rag(query))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408a6f8a-045f-4905-aa4f-2fbc688a70ea",
   "metadata": {},
   "source": [
    "# ‚Üë FUCK!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2aa07d3-e934-442e-ad44-74e1df6f3f82",
   "metadata": {},
   "source": [
    "# ==================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a5cef13c-99a0-42a3-9227-0ef8ed853362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Total Documents: 2\n",
      "üîπ Total Chunks: 243\n"
     ]
    }
   ],
   "source": [
    "def count_db_entries():\n",
    "    \"\"\"Returns the number of stored documents and chunks in the FAISS database.\"\"\"\n",
    "    db = create_or_load_db()\n",
    "    \n",
    "    if db is None:\n",
    "        return \"‚ö†Ô∏è No database found. Please add documents first.\"\n",
    "\n",
    "    # Number of stored chunks (each vector represents a chunk)\n",
    "    num_chunks = db.index.ntotal  \n",
    "\n",
    "    # If metadata is available, count unique documents\n",
    "    sources = set(doc.metadata.get(\"source\") for doc in db.docstore._dict.values())\n",
    "\n",
    "    return f\"üìÑ Total Documents: {len(sources)}\\nüîπ Total Chunks: {num_chunks}\"\n",
    "\n",
    "# Run this to check the count\n",
    "print(count_db_entries())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
